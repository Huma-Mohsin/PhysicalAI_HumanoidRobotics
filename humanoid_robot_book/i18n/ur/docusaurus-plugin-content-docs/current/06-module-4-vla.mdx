---
id: module-4-vla
title: "ماڈیول 4: Vision-Language-Action (VLA)"
sidebar_label: "ماڈیول 4: VLA"
sidebar_position: 6
---

import PersonalizeButton from '@site/src/components/Personalization/PersonalizeButton';
import ContentVariant from '@site/src/components/Personalization/ContentVariant';


# ماڈیول 4: Vision-Language-Action (VLA)

## جائزہ

**توجہ**: LLMs اور روبوٹکس کا انضمام

Vision-Language-Action (VLA) ماڈلز embodied AI کی جدید ترین شکل ہیں—ایسے سسٹم جو قدرتی زبان کی کمانڈز کو سمجھ سکتے ہیں، اپنے ماحول کو بصارت کے ذریعے محسوس کر سکتے ہیں، اور جسمانی اعمال انجام دے سکتے ہیں۔ یہ ماڈیول آپ کو سکھاتا ہے کہ گفتگو کرنے والے روبوٹ کیسے بنائیں جو آوازی کمانڈز کا جواب دیتے ہیں اور خودمختار طور پر پیچیدہ کام انجام دیتے ہیں۔

---

## سیکھنے کے مقاصد

اس ماڈیول کے اختتام تک، آپ یہ کر سکیں گے:

1. ✅ OpenAI Whisper کے ساتھ تقریر کی شناخت کو ضم کرنا
2. ✅ علمی منصوبہ بندی کے لیے LLMs (GPT-4) استعمال کرنا
3. ✅ بصارت پر مبنی چیزوں کی شناخت نافذ کرنا
4. ✅ قدرتی زبان کو ROS 2 اعمال میں ترجمہ کرنا
5. ✅ Capstone پروجیکٹ بنانا: خودمختار ہیومنائیڈ
6. ✅ کثیر الجہتی تعامل کے سسٹم ڈیپلائے کرنا

---

<PersonalizeButton />

---

## VLA پائپ لائن

```
┌──────────────┐     ┌────────────┐     ┌──────────────┐     ┌────────────┐
│ Voice Command│────▶│  Whisper   │────▶│   GPT-4      │────▶│ ROS 2      │
│ "Clean room" │     │  (Speech   │     │  (Planning)  │     │ Actions    │
│              │     │  to Text)  │     │              │     │            │
└──────────────┘     └────────────┘     └──────────────┘     └────────────┘
                                                │
                                                │
                                        ┌───────▼────────┐
                                        │  Vision System  │
                                        │  (Object        │
                                        │   Detection)    │
                                        └─────────────────┘
```

---

## جزو 1: OpenAI Whisper کے ساتھ آواز سے متن

Whisper OpenAI کا جدید ترین تقریر شناخت ماڈل ہے۔

<ContentVariant hardwareType="gpu_workstation">

### Whisper انسٹال کریں (GPU Accelerated)

**RTX 4090 صارفین کے لیے - زیادہ سے زیادہ کارکردگی:**

```bash
# GPU acceleration کے لیے CUDA support کے ساتھ انسٹال کریں
pip install openai-whisper
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### مقامی ماڈل ٹریننگ

```python
import whisper

# بہترین درستگی کے لیے سب سے بڑا ماڈل استعمال کریں (10GB+ VRAM درکار ہے)
model = whisper.load_model("large-v2")  # GPU ضروری ہے

# GPU acceleration کے ساتھ نقل کریں
result = model.transcribe("audio.mp3", fp16=True)  # رفتار کے لیے FP16 استعمال کریں
print(result["text"])
```

**کارکردگی**: CPU سے ~10x تیز، حقیقی وقت میں نقل ممکن ہے

</ContentVariant>

<ContentVariant hardwareType="edge_device">

### Whisper انسٹال کریں (Jetson Optimized)

**Jetson Orin Nano کے لیے - ہلکے ماڈلز:**

```bash
# ARM64 کے لیے بہتر بنایا گیا ورژن انسٹال کریں
pip install openai-whisper

# Jetson کی میموری میں فٹ ہونے کے لیے چھوٹا ماڈل استعمال کریں
```

### Jetson پر ڈیپلائمنٹ

```python
import whisper

# Jetson کے لیے چھوٹا ماڈل (base یا small) استعمال کریں
model = whisper.load_model("base")  # 74M parameters، ~500MB

# کم میموری استعمال کے ساتھ نقل کریں
result = model.transcribe("audio.mp3", fp16=False)  # استحکام کے لیے FP32
print(result["text"])
```

**حدود**:
- صرف "tiny" یا "base" ماڈلز استعمال کریں (بڑے ماڈلز فٹ نہیں ہوں گے)
- GPU workstations سے 2-3x سست کی توقع کریں
- حقیقی وقت میں نقل مشکل ہے

**تجویز**: آڈیو کو cloud میں پہلے سے پروسیس کریں، Jetson پر inference ڈیپلائے کریں

</ContentVariant>

<ContentVariant hardwareType="cloud_mac">

### Whisper انسٹال کریں (Cloud/Mac)

**Mac/Cloud صارفین کے لیے - API پر مبنی طریقہ:**

```bash
# آپشن 1: مقامی CPU انسٹالیشن (سست لیکن کام کرتا ہے)
pip install openai-whisper

# آپشن 2: OpenAI API استعمال کریں (Mac کے لیے تجویز کردہ)
pip install openai
```

### OpenAI API استعمال کریں (تجویز کردہ)

```python
from openai import OpenAI
client = OpenAI(api_key="your-api-key")

# Cloud-based نقل (مقامی GPU کی ضرورت نہیں)
with open("audio.mp3", "rb") as audio_file:
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file
    )
print(transcript.text)
```

**متبادل**: ٹریننگ کے لیے GPU کے ساتھ Google Colab استعمال کریں

```python
# Colab notebook میں:
!pip install openai-whisper
import whisper
model = whisper.load_model("large-v2")  # Colab کے مفت GPU استعمال کرتا ہے
```

**فوائد**: مقامی GPU کی ضرورت نہیں، prototyping کے لیے لاگت مؤثر

</ContentVariant>

---

### حقیقی وقت میں ROS 2 انضمام

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from audio_common_msgs.msg import AudioData
import whisper
import numpy as np
import io
import wave

class VoiceCommandNode(Node):
    def __init__(self):
        super().__init__('voice_command_node')

        # Whisper ماڈل لوڈ کریں
        self.model = whisper.load_model("base")

        # مائیکروفون آڈیو کو subscribe کریں
        self.audio_sub = self.create_subscription(
            AudioData,
            '/audio',
            self.audio_callback,
            10
        )

        # نقل شدہ کمانڈز شائع کریں
        self.command_pub = self.create_publisher(
            String,
            '/voice_command',
            10
        )

        self.audio_buffer = []
        self.buffer_duration = 3.0  # سیکنڈ

    def audio_callback(self, msg):
        # آڈیو ڈیٹا جمع کریں
        self.audio_buffer.extend(msg.data)

        # جب buffer بھر جائے تو پروسیس کریں
        if len(self.audio_buffer) >= 48000 * self.buffer_duration:
            self.process_audio()
            self.audio_buffer = []

    def process_audio(self):
        # numpy array میں تبدیل کریں
        audio_data = np.array(self.audio_buffer, dtype=np.float32)
        audio_data = audio_data / 32768.0  # Normalize کریں

        # Whisper کے ساتھ نقل کریں
        result = self.model.transcribe(
            audio_data,
            language='en',
            fp16=False
        )

        command = result["text"].strip()

        if command:
            self.get_logger().info(f'آوازی کمانڈ: {command}')

            # کمانڈ شائع کریں
            msg = String()
            msg.data = command
            self.command_pub.publish(msg)

def main(args=None):
    rclpy.init(args=args)
    node = VoiceCommandNode()
    rclpy.spin(node)
    rclpy.shutdown()
```

---

## جزو 2: GPT-4 کے ساتھ علمی منصوبہ بندی

LLMs اعلیٰ سطح کی کمانڈز کو روبوٹ کے اعمال کی ترتیب میں ترجمہ کر سکتے ہیں۔

### کام کی تقسیم کی مثال

**ان پٹ**: "کمرہ صاف کرو"

**GPT-4 آؤٹ پٹ**:
```json
{
  "task": "clean_room",
  "steps": [
    {"action": "navigate_to", "target": "table", "priority": 1},
    {"action": "detect_objects", "category": "trash", "priority": 2},
    {"action": "pick_object", "object_id": "trash_1", "priority": 3},
    {"action": "navigate_to", "target": "trash_bin", "priority": 4},
    {"action": "place_object", "target": "trash_bin", "priority": 5},
    {"action": "return_to", "target": "charging_station", "priority": 6}
  ]
}
```

---

### GPT-4 کے ساتھ ROS 2 انضمام

```python
import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from std_msgs.msg import String
from geometry_msgs.msg import PoseStamped
from nav2_msgs.action import NavigateToPose
import openai
import json

class CognitivePlannerNode(Node):
    def __init__(self):
        super().__init__('cognitive_planner')

        # OpenAI client
        self.client = openai.OpenAI()

        # آوازی کمانڈز کو subscribe کریں
        self.command_sub = self.create_subscription(
            String,
            '/voice_command',
            self.command_callback,
            10
        )

        # Navigation action client
        self.nav_client = ActionClient(
            self,
            NavigateToPose,
            'navigate_to_pose'
        )

        # معلوم مقامات
        self.locations = {
            "table": {"x": 2.0, "y": 3.0},
            "trash_bin": {"x": -1.0, "y": -2.0},
            "charging_station": {"x": 0.0, "y": 0.0}
        }

    def command_callback(self, msg):
        command = msg.data
        self.get_logger().info(f'کمانڈ پروسیس کر رہے ہیں: {command}')

        # GPT-4 سے action plan حاصل کریں
        plan = self.generate_plan(command)

        # منصوبہ نافذ کریں
        self.execute_plan(plan)

    def generate_plan(self, command):
        """GPT-4 استعمال کرکے action plan بنائیں۔"""

        system_prompt = """You are a robot task planner. Given a high-level command,
        break it down into a sequence of robot actions. Available actions:
        - navigate_to(target): Move to a location
        - detect_objects(category): Find objects in the environment
        - pick_object(object_id): Pick up an object
        - place_object(target): Place object at target

        Available locations: table, trash_bin, charging_station

        Output JSON format:
        {
          "task": "task_name",
          "steps": [
            {"action": "navigate_to", "target": "location", "priority": 1},
            ...
          ]
        }
        """

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": command}
            ],
            response_format={"type": "json_object"}
        )

        plan_json = response.choices[0].message.content
        plan = json.loads(plan_json)

        self.get_logger().info(f'بنایا گیا منصوبہ: {json.dumps(plan, indent=2)}')
        return plan

    def execute_plan(self, plan):
        """قدم بہ قدم action plan نافذ کریں۔"""

        for step in sorted(plan["steps"], key=lambda x: x["priority"]):
            action = step["action"]

            if action == "navigate_to":
                self.navigate_to(step["target"])
            elif action == "detect_objects":
                self.detect_objects(step["category"])
            elif action == "pick_object":
                self.pick_object(step["object_id"])
            elif action == "place_object":
                self.place_object(step["target"])

    def navigate_to(self, target):
        """ہدف کے مقام پر نیویگیٹ کریں۔"""

        if target not in self.locations:
            self.get_logger().error(f'نامعلوم مقام: {target}')
            return

        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.pose.position.x = self.locations[target]["x"]
        goal_msg.pose.pose.position.y = self.locations[target]["y"]
        goal_msg.pose.pose.orientation.w = 1.0

        self.get_logger().info(f'{target} کی طرف نیویگیٹ کر رہے ہیں...')

        self.nav_client.wait_for_server()
        future = self.nav_client.send_goal_async(goal_msg)
        future.add_done_callback(self.goal_response_callback)

    def goal_response_callback(self, future):
        goal_handle = future.result()

        if not goal_handle.accepted:
            self.get_logger().error('ہدف مسترد ہو گیا')
            return

        self.get_logger().info('ہدف قبول ہو گیا')
        result_future = goal_handle.get_result_async()
        result_future.add_done_callback(self.get_result_callback)

    def get_result_callback(self, future):
        result = future.result().result
        self.get_logger().info('نیویگیشن مکمل ہو گئی')
```

---

## جزو 3: بصارت پر مبنی چیزوں کی شناخت

بصری ادراک کے لیے YOLO یا دیگر object detection ماڈلز کو ضم کریں۔

### ROS 2 کے ساتھ YOLOv8

```python
from ultralytics import YOLO
import cv2
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray, Detection2D
from cv_bridge import CvBridge

class ObjectDetectorNode(Node):
    def __init__(self):
        super().__init__('object_detector')

        # YOLO ماڈل لوڈ کریں
        self.model = YOLO('yolov8n.pt')

        # CV Bridge
        self.bridge = CvBridge()

        # کیمرے کو subscribe کریں
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10
        )

        # شناخت شدہ چیزیں شائع کریں
        self.detection_pub = self.create_publisher(
            Detection2DArray,
            '/detections',
            10
        )

    def image_callback(self, msg):
        # ROS Image کو OpenCV میں تبدیل کریں
        cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")

        # Inference چلائیں
        results = self.model(cv_image)

        # Detection message بنائیں
        detection_array = Detection2DArray()
        detection_array.header = msg.header

        for result in results:
            for box in result.boxes:
                detection = Detection2D()

                # Bounding box
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                detection.bbox.center.x = (x1 + x2) / 2
                detection.bbox.center.y = (y1 + y2) / 2
                detection.bbox.size_x = x2 - x1
                detection.bbox.size_y = y2 - y1

                # Class اور confidence
                detection.results.append({
                    "id": int(box.cls[0]),
                    "score": float(box.conf[0])
                })

                detection_array.detections.append(detection)

        # شناخت شدہ چیزیں شائع کریں
        self.detection_pub.publish(detection_array)
```

---

## Capstone پروجیکٹ: خودمختار ہیومنائیڈ

### پروجیکٹ کا جائزہ

ایک مکمل خودمختار ہیومنائیڈ سسٹم بنائیں جو:

1. **آوازی کمانڈز وصول کرے** ("کمرہ صاف کرو"، "مجھے پانی لاؤ")
2. **GPT-4 استعمال کرکے اعمال کی ترتیب کی منصوبہ بندی کرے**
3. **Nav2 استعمال کرکے خودمختار طور پر نیویگیٹ کرے**
4. **vision ماڈلز کے ساتھ چیزوں کی شناخت کرے**
5. **بازو کے کنٹرول کے ساتھ چیزوں کو ہیرا پھیری کرے**
6. **تقریر کی ترکیب کے ذریعے رائے فراہم کرے**

---

### سسٹم آرکیٹیکچر

```
┌─────────────────────────────────────────────────────────────┐
│                  خودمختار ہیومنائیڈ سسٹم                     │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│  ┌──────────────┐      ┌──────────────┐      ┌───────────┐ │
│  │   Whisper    │─────▶│    GPT-4     │─────▶│   Nav2    │ │
│  │ (Voice Input)│      │  (Planning)  │      │(Navigation)│ │
│  └──────────────┘      └──────────────┘      └───────────┘ │
│                              │                               │
│                              ▼                               │
│                     ┌─────────────────┐                     │
│                     │  YOLOv8/CLIP    │                     │
│                     │ (Object Detect) │                     │
│                     └─────────────────┘                     │
│                              │                               │
│                              ▼                               │
│                     ┌─────────────────┐                     │
│                     │   MoveIt2       │                     │
│                     │(Manipulation)   │                     │
│                     └─────────────────┘                     │
│                                                               │
└─────────────────────────────────────────────────────────────┘
```

---

### ضروریات

**ہارڈویئر:**
- سمیولیشن کے لیے RTX 4080+ workstation
- ڈیپلائمنٹ کے لیے NVIDIA Jetson Orin
- Intel RealSense D435i کیمرہ
- ReSpeaker مائیکروفون array
- Unitree G1 humanoid (یا سمیولیشن)

**سافٹ ویئر:**
- سمیولیشن کے لیے Isaac Sim
- ROS 2 Humble
- Isaac ROS packages
- OpenAI API access

---

### نفاذ کے قدمات

#### قدم 1: آوازی کمانڈ پروسیسنگ

```python
# voice_processor.launch.py
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        Node(
            package='audio_capture',
            executable='audio_capture_node',
            parameters=[{
                'device': 'plughw:CARD=seeed8micvoicec,DEV=0',
                'format': 1,
                'channels': 8,
                'depth': 16,
                'sample_rate': 16000
            }]
        ),
        Node(
            package='my_robot',
            executable='voice_command_node',
            output='screen'
        ),
    ])
```

#### قدم 2: علمی منصوبہ بندی

```python
# Cognitive planner چلائیں
ros2 run my_robot cognitive_planner_node
```

#### قدم 3: ادراک کی پائپ لائن

```bash
# Isaac ROS VSLAM + Object Detection لانچ کریں
ros2 launch my_robot perception.launch.py
```

#### قدم 4: نیویگیشن

```bash
# Nav2 لانچ کریں
ros2 launch my_robot navigation.launch.py
```

#### قدم 5: انضمام

```bash
# مکمل سسٹم لانچ کریں
ros2 launch my_robot autonomous_humanoid.launch.py
```

---

## کثیر الجہتی تعامل

### رائے کے لیے تقریر کی ترکیب

```python
from gtts import gTTS
import os

class RobotSpeaker(Node):
    def __init__(self):
        super().__init__('robot_speaker')

        self.status_sub = self.create_subscription(
            String,
            '/robot_status',
            self.status_callback,
            10
        )

    def status_callback(self, msg):
        self.speak(msg.data)

    def speak(self, text):
        # تقریر بنائیں
        tts = gTTS(text=text, lang='en')
        tts.save("output.mp3")

        # آڈیو چلائیں
        os.system("mpg321 output.mp3")
        os.remove("output.mp3")
```

---

## تشخیص کا معیار

### Capstone پروجیکٹ گریڈنگ (100 پوائنٹس)

| زمرہ | پوائنٹس | ضروریات |
|----------|--------|--------------|
| **آوازی کمانڈ پروسیسنگ** | 20 | Whisper انضمام، درستگی >90% |
| **علمی منصوبہ بندی** | 25 | GPT-4 کام کی تقسیم، JSON آؤٹ پٹ کی تصدیق |
| **نیویگیشن** | 20 | رکاوٹوں سے بچنے کے ساتھ خودمختار نیویگیشن |
| **چیزوں کی شناخت** | 15 | YOLOv8 انضمام، شناخت کی درستگی >80% |
| **ہیرا پھیری** | 10 | اٹھانے اور رکھنے کی فعالیت |
| **سسٹم انضمام** | 10 | تمام اجزاء مل کر کام کر رہے ہیں |

**بونس** (+10): جسمانی روبوٹ پر Sim-to-real ڈیپلائمنٹ

---

## وسائل

- [OpenAI Whisper](https://github.com/openai/whisper)
- [GPT-4 API دستاویزات](https://platform.openai.com/docs/api-reference)
- [YOLOv8](https://github.com/ultralytics/ultralytics)
- [MoveIt2 دستاویزات](https://moveit.ros.org/)
- [Nav2 دستاویزات](https://navigation.ros.org/)

---

## مبارک ہو!

آپ نے **فزیکل AI اور ہیومنائیڈ روبوٹکس** کورس مکمل کر لیا!

اب آپ کے پاس یہ مہارتیں ہیں:
- ✅ ROS 2 روبوٹ کنٹرولرز بنانا
- ✅ Gazebo اور Isaac Sim میں روبوٹس سمیولیٹ کرنا
- ✅ AI سے چلنے والا ادراک نافذ کرنا
- ✅ گفتگو کرنے والے ہیومنائیڈ روبوٹس بنانا
- ✅ حقیقی ہارڈویئر پر ڈیپلائے کرنا

### اگلا کیا ہے؟

- ROS کمیونٹی میں شامل ہوں
- اوپن سورس روبوٹکس پروجیکٹس میں حصہ ڈالیں
- اپنا روبوٹ startup بنائیں
- جدید موضوعات پر تحقیق کریں (کثیر روبوٹ سسٹم، swarm robotics، انسان-روبوٹ تعاون)

**فزیکل AI کے مستقبل میں خوش آمدید!**
