---
id: week-12-13-conversational
title: "ูุช 12-13: VLA ฺฉ ุณุงุชฺพ ฺฏูุชฺฏู ฺฉุฑู ูุงู ุฑูุจููนฺฉุณ"
sidebar_label: "ูุช 12-13: ฺฏูุชฺฏู AI"
sidebar_position: 7
---

import PersonalizeButton from '@site/src/components/Personalization/PersonalizeButton';
import ContentVariant from '@site/src/components/Personalization/ContentVariant';


# ูุช 12-13: Vision-Language-Action ูุงฺูุฒ ฺฉ ุณุงุชฺพ ฺฏูุชฺฏู ฺฉุฑู ูุงู ุฑูุจููนฺฉุณ

## ุฌุงุฆุฒ

**ุชูุฌ**: VLA (Vision-Language-Action) ูุงฺูุฒ ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ุขูุงุฒ ุณ ฺฉููนุฑูู ูู ูุงู ูููุงุฆฺ ุฑูุจููนุณ

ุงุณ ุขุฎุฑ ูุงฺูู ูฺบุ ุขูพ ุฑ ฺุฒ ฺฉู ฺฉุฌุง ฺฉุฑ ฺฉ ุงฺฉ ฺฏูุชฺฏู ฺฉุฑู ูุงูุง ูููุงุฆฺ ุฑูุจููน ุจูุงุฆฺบ ฺฏ ุฌู:
- ุขูุงุฒ ฺฉ ุงุญฺฉุงูุงุช ุณูุชุง  (Whisper ASR)
- ุจุตุงุฑุช ุงูุฑ ุฒุจุงู ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ุณุงู ู ุณุจุงู ุณูุฌฺพุชุง  (Gemini VLA)
- ูุงุจู ุนูู ุฑูุจููน ุงฺฉุดูุฒ ุจูุงุชุง  (ROS 2 ุงุญฺฉุงูุงุช)
- ุณูููุดู ุงูุฑ ุญูู ุงุฑฺูุฆุฑ ูฺบ ูุญููุธ ุทุฑู ุณ ุญุฑฺฉุชฺบ ุงูุฌุงู ุฏุชุง 

---

## ุณฺฉฺพู ฺฉ ููุงุตุฏ

ุงุณ ูุงฺูู ฺฉ ุงุฎุชุชุงู ุชฺฉุ ุขูพ  ฺฉุฑ ุณฺฉฺบ ฺฏ:

1. โ ุญูู ููุช ูฺบ ุชูุฑุฑ ฺฉ ุดูุงุฎุช ฺฉ ู Whisper ฺฉู ุถู ฺฉุฑูุง
2. โ ุงฺฉุดู ูพูุงููฺฏ ฺฉ ู Google Gemini ฺฉู VLA ูุงฺู ฺฉ ุทูุฑ ูพุฑ ุฌูฺูุง
3. โ ูุฏุฑุช ุฒุจุงู ฺฉู ุฑูุจููน ุงฺฉุดูุฒ ูฺบ ุชุจุฏู ฺฉุฑูุง
4. โ ุขูุงุฒ ุณ ฺฉููนุฑูู ูู ูุงูุง ููฺฏุดู ุณุณูนู ุจูุงูุง
5. โ ุงุฑฺูุฆุฑ ุงฺฉุณูุฑุดู ฺฉ ุณุงุชฺพ Jetson ูพุฑ ฺูพูุงุฆ ฺฉุฑูุง
6. โ Isaac Sim ูฺบ ูนุณูน ฺฉุฑูุง ุงูุฑ ุญูู ุฑูุจููน ูฺบ ููุชูู ฺฉุฑูุง

---

<PersonalizeButton />

## VLA ูพุงุฆูพ ูุงุฆู

```
โโโโโโโโโโโโโโโโ
โ Voice Input  โ  "Pick up the red cube"
โโโโโโโโฌโโโโโโโโ
       โ
โโโโโโโโผโโโโโโโโ
โ   Whisper    โ  Speech โ Text
โโโโโโโโฌโโโโโโโโ
       โ
โโโโโโโโผโโโโโโโโ
โ Gemini VLA   โ  Text + Vision โ Action Plan
โโโโโโโโฌโโโโโโโโ
       โ
โโโโโโโโผโโโโโโโโ
โ ROS 2 Action โ  Execute motion
โโโโโโโโโโโโโโโโ
```

---

## ุญุต 1: Whisper ฺฉ ุณุงุชฺพ ุชูุฑุฑ ฺฉ ุดูุงุฎุช

OpenAI ฺฉุง Whisper ุงฺฉ ูุถุจูุท ุชูุฑุฑ ุดูุงุฎุช ูุงฺู  ุฌู 99 ุฒุจุงููฺบ ฺฉู ุณูพูุฑูน ฺฉุฑุชุง 

### Whisper ุงูุณูนุงู ฺฉุฑูุง

<ContentVariant hardwareType="gpu_workstation">

#### GPU ูุฑฺฉ ุณูนุดู ุตุงุฑูู ฺฉ ู (RTX 4090+)

**GPU-Accelerated ุงูุณูนุงูุดู**

```bash
# PyTorch ฺฉู CUDA ฺฉ ุณุงุชฺพ ุงูุณูนุงู ฺฉุฑฺบ
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Whisper ุงูุณูนุงู ฺฉุฑฺบ
pip install openai-whisper

# ุขฺู ูพุฑูุณุณูฺฏ ุงูุณูนุงู ฺฉุฑฺบ
sudo apt-get install ffmpeg
pip install pyaudio

# GPU ุฏุณุชุงุจ ูนุณูน ฺฉุฑฺบ
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

**ฺฉุงุฑฺฉุฑุฏฺฏ**: RTX 4090 ูพุฑ `whisper-large-v3` ฺฉ ุณุงุชฺพ ุญูู ููุช ูฺบ ูนุฑุงูุณฺฉุฑูพุดู

</ContentVariant>

<ContentVariant hardwareType="edge_device">

#### Jetson Orin Nano ุตุงุฑูู ฺฉ ู

**Jetson-Optimized ุงูุณูนุงูุดู**

```bash
# Jetson ฺฉ ู PyTorch ุงูุณูนุงู ฺฉุฑฺบ (ARM64)
pip install torch torchvision torchaudio

# Whisper ุงูุณูนุงู ฺฉุฑฺบ (Jetson ฺฉ ู ฺฺพููนุง ูุงฺู ุงุณุชุนูุงู ฺฉุฑฺบ)
pip install openai-whisper

# ุขฺู ูุงุฆุจุฑุฑุฒ ุงูุณูนุงู ฺฉุฑฺบ
sudo apt-get install portaudio19-dev python3-pyaudio
pip install pyaudio

# ุญูู ููุช ฺฉ ฺฉุงุฑฺฉุฑุฏฺฏ ฺฉ ู whisper-base ุง whisper-small ุงุณุชุนูุงู ฺฉุฑฺบ
```

**ุชุฌูุฒ**: Jetson ูพุฑ 2-3x ุชุฒ ุชุฑ ุงููุฑูุณ ฺฉ ู `whisper-base` ูุงฺู ุงุณุชุนูุงู ฺฉุฑฺบ

</ContentVariant>

<ContentVariant hardwareType="cloud_mac">

#### Cloud / Mac ุตุงุฑูู ฺฉ ู

**CPU ุงูุณูนุงูุดู**

```bash
# PyTorch ุงูุณูนุงู ฺฉุฑฺบ (CPU ูุฑฺู)
pip install torch torchvision torchaudio

# Whisper ุงูุณูนุงู ฺฉุฑฺบ
pip install openai-whisper

# Mac: ุขฺู ูุงุฆุจุฑุฑุฒ ุงูุณูนุงู ฺฉุฑฺบ
brew install portaudio
pip install pyaudio

# CPU ฺฉ ู whisper-tiny ุง whisper-base ุงุณุชุนูุงู ฺฉุฑฺบ
```

**ูุชุจุงุฏู**: ููุงู GPU ฺฉ ุจุบุฑ ุชุฒ ุชุฑ ุงููุฑูุณ ฺฉ ู Google Cloud Speech-to-Text API ุงุณุชุนูุงู ฺฉุฑฺบ

</ContentVariant>

---

### ุญูู ููุช ูฺบ ุชูุฑุฑ ฺฉ ุดูุงุฎุช

```python
import whisper
import pyaudio
import wave
import tempfile

class VoiceListener:
    def __init__(self, model_size="base"):
        """
        model_size: tiny, base, small, medium, large
        GPU ุตุงุฑูู: ุจุชุฑู ุฏุฑุณุชฺฏ ฺฉ ู 'large-v3' ุงุณุชุนูุงู ฺฉุฑฺบ
        Jetson ุตุงุฑูู: ุญูู ููุช ฺฉ ฺฉุงุฑฺฉุฑุฏฺฏ ฺฉ ู 'base' ุงุณุชุนูุงู ฺฉุฑฺบ
        """
        self.model = whisper.load_model(model_size)
        self.audio = pyaudio.PyAudio()

    def record_audio(self, duration=5):
        """ูุงุฆฺฉุฑูููู ุณ ุขฺู ุฑฺฉุงุฑฺ ฺฉุฑฺบ"""
        CHUNK = 1024
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        RATE = 16000

        stream = self.audio.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=RATE,
            input=True,
            frames_per_buffer=CHUNK
        )

        print("๐ค ุณู ุฑุง ...")
        frames = []

        for _ in range(0, int(RATE / CHUNK * duration)):
            data = stream.read(CHUNK)
            frames.append(data)

        stream.stop_stream()
        stream.close()

        # ุนุงุฑุถ WAV ูุงุฆู ูฺบ ูุญููุธ ฺฉุฑฺบ
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            wf = wave.open(f.name, 'wb')
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(self.audio.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(frames))
            wf.close()
            return f.name

    def transcribe(self, audio_file):
        """ุขฺู ฺฉู ูุชู ูฺบ ุชุจุฏู ฺฉุฑฺบ"""
        result = self.model.transcribe(audio_file)
        return result["text"]

    def listen_and_transcribe(self, duration=5):
        """ุงฺฉ ฺฉุงู ูฺบ ุฑฺฉุงุฑฺ ุงูุฑ ูนุฑุงูุณฺฉุฑุงุฆุจ ฺฉุฑฺบ"""
        audio_file = self.record_audio(duration)
        text = self.transcribe(audio_file)
        print(f"๐ ูนุฑุงูุณฺฉุฑุงุฆุจ ุดุฏ: {text}")
        return text

# ุงุณุชุนูุงู
listener = VoiceListener(model_size="base")
command = listener.listen_and_transcribe(duration=5)
print(f"ุตุงุฑู ู ฺฉุง: {command}")
```

---

## ุญุต 2: Google Gemini ฺฉ ุณุงุชฺพ VLA

Google Gemini ุจุตุงุฑุช ุงูุฑ ุฒุจุงู ฺฉู ฺฉุฌุง ฺฉุฑ ฺฉ ุฑูุจููน ุงฺฉุดูุฒ ุจูุงุชุง 

### Gemini API ุณูน ุงูพ ฺฉุฑูุง

```bash
# Gemini SDK ุงูุณูนุงู ฺฉุฑฺบ
pip install google-generativeai

# API key ุณูน ฺฉุฑฺบ
export GEMINI_API_KEY="your-api-key-here"
```

**ุงูพู API key ุญุงุตู ฺฉุฑฺบ**: https://makersuite.google.com/app/apikey

---

### VLA Prompt ุงูุฌูุฆุฑูฺฏ

```python
import google.generativeai as genai
import os

class VLAController:
    def __init__(self):
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = genai.GenerativeModel('gemini-1.5-pro')

        # VLA ฺฉ ู ุณุณูนู prompt
        self.system_prompt = """You are a humanoid robot control system.
Convert natural language commands into ROS 2 action sequences.

Available actions:
- navigate(x, y, theta): Move to position (x, y) with orientation theta
- grasp(object_name): Pick up an object
- place(x, y, z): Place object at position
- wave(): Wave hand
- turn(degrees): Rotate in place
- speak(text): Say something

Output format (JSON):
{
  "actions": [
    {"type": "navigate", "params": {"x": 2.0, "y": 1.0, "theta": 0.0}},
    {"type": "grasp", "params": {"object": "red_cube"}}
  ],
  "explanation": "Brief explanation of the plan"
}

Only output valid JSON. No other text."""

    def plan_actions(self, command, image=None):
        """
        ุขูุงุฒ ฺฉ ุญฺฉู ฺฉู ุงฺฉุดู ุณฺฉููุณ ูฺบ ุชุจุฏู ฺฉุฑฺบ

        Args:
            command: ูุฏุฑุช ุฒุจุงู ฺฉุง ุญฺฉู (Whisper ุณ)
            image: ุงุฎุชุงุฑ ฺฉูุฑ ุชุตูุฑ (PIL Image ุง ูุงุฆู ูพุงุชฺพ)
        """
        prompt = f"{self.system_prompt}\n\nUser command: {command}"

        if image:
            # Multimodal input (ูุชู + ุชุตูุฑ)
            response = self.model.generate_content([prompt, image])
        else:
            # ุตุฑู ูุชู input
            response = self.model.generate_content(prompt)

        return response.text

# ุงุณุชุนูุงู
vla = VLAController()

# ุตุฑู ูุชู ฺฉุง ุญฺฉู
command = "Go to the kitchen and pick up the red mug"
actions = vla.plan_actions(command)
print(actions)

# ุจุตุงุฑุช ฺฉ ุณุงุชฺพ (multimodal)
from PIL import Image
camera_image = Image.open("camera_feed.jpg")
command = "Pick up the object in front of me"
actions = vla.plan_actions(command, image=camera_image)
print(actions)
```

**ูุชููุน ุขุคูน ูพูน:**

```json
{
  "actions": [
    {"type": "navigate", "params": {"x": 5.0, "y": 3.0, "theta": 1.57}},
    {"type": "grasp", "params": {"object": "red_mug"}}
  ],
  "explanation": "Navigate to kitchen coordinates, then grasp the red mug"
}
```

---

## ุญุต 3: ROS 2 ฺฉ ุณุงุชฺพ ุงฺฉุดู ุงุฌุฑุงุฆ

JSON ุงฺฉุดูุฒ ฺฉู ROS 2 ุงุญฺฉุงูุงุช ูฺบ ุชุจุฏู ฺฉุฑฺบ

```python
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PoseStamped
from nav2_msgs.action import NavigateToPose
from rclpy.action import ActionClient
import json

class ActionExecutor(Node):
    def __init__(self):
        super().__init__('action_executor')

        # Navigation action client
        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')

    def execute_actions(self, actions_json):
        """VLA ุณ ุงฺฉุดู ุณฺฉููุณ ุงูุฌุงู ุฏฺบ"""
        actions = json.loads(actions_json)

        for action in actions['actions']:
            action_type = action['type']
            params = action['params']

            if action_type == 'navigate':
                self.navigate_to(params['x'], params['y'], params['theta'])
            elif action_type == 'grasp':
                self.grasp_object(params['object'])
            elif action_type == 'wave':
                self.wave_hand()
            # ูุฒุฏ ุงฺฉุดูุฒ ุดุงูู ฺฉุฑฺบ...

    def navigate_to(self, x, y, theta):
        """Nav2 ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ูพูุฒุดู ูพุฑ ุฌุงุฆฺบ"""
        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.pose.position.x = x
        goal_msg.pose.pose.position.y = y

        # theta ฺฉู quaternion ูฺบ ุชุจุฏู ฺฉุฑฺบ
        from tf_transformations import quaternion_from_euler
        q = quaternion_from_euler(0, 0, theta)
        goal_msg.pose.pose.orientation.z = q[2]
        goal_msg.pose.pose.orientation.w = q[3]

        self.get_logger().info(f'({x}, {y}, {theta}) ูพุฑ ุฌุง ุฑ ฺบ')
        self.nav_client.send_goal_async(goal_msg)

    def grasp_object(self, object_name):
        """ฺุฒ ูพฺฉฺฺบ (ุขุณุงู ุจูุงุง ฺฏุง)"""
        self.get_logger().info(f'{object_name} ฺฉู ูพฺฉฺ ุฑ ฺบ')
        # ุงฺบ ูพฺฉฺู ฺฉ ููุทู ูุงฺฏู ฺฉุฑฺบ
        # - ุจุงุฒู ฺฉู pre-grasp pose ูฺบ ู ุฌุงุฆฺบ
        # - ฺฏุฑูพุฑ ุจูุฏ ฺฉุฑฺบ
        # - ฺุฒ ุงูนฺพุงุฆฺบ

    def wave_hand(self):
        """ุงุชฺพ ูุงู ฺฉุง ุงุดุงุฑ"""
        self.get_logger().info('ุงุชฺพ ูุง ุฑ ฺบ')
        # ุงุชฺพ ูุงู ฺฉ ุญุฑฺฉุช ฺฉ ู joint trajectory ุดุงุฆุน ฺฉุฑฺบ

# ูู ูููพ
def main():
    rclpy.init()

    # ุงุฌุฒุงุก ฺฉู ุดุฑูุน ฺฉุฑฺบ
    listener = VoiceListener(model_size="base")
    vla = VLAController()
    executor = ActionExecutor()

    print("๐ค ฺฏูุชฺฏู ฺฉุฑู ูุงูุง ุฑูุจููน ุชุงุฑ !")

    while True:
        # ุขูุงุฒ ฺฉ ุญฺฉู ฺฉ ู ุณูฺบ
        command = listener.listen_and_transcribe(duration=5)

        if "stop" in command.lower():
            break

        # VLA ฺฉ ุณุงุชฺพ ุงฺฉุดู ูพูุงู ุจูุงุฆฺบ
        actions_json = vla.plan_actions(command)
        print(f"ุงฺฉุดู ูพูุงู: {actions_json}")

        # ุงฺฉุดูุฒ ุงูุฌุงู ุฏฺบ
        executor.execute_actions(actions_json)

    executor.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

## ุญุต 4: ูฺฉูู VLA ุณุณูนู

### ูฺฉูู ุงููนฺฏุฑุดู ูุซุงู

```python
#!/usr/bin/env python3
"""
ฺฏูุชฺฏู ฺฉุฑู ูุงูุง ูููุงุฆฺ ุฑูุจููน
Whisper (ASR) + Gemini (VLA) + ROS 2 (Execution) ฺฉู ฺฉุฌุง ฺฉุฑุชุง 
"""

import rclpy
from rclpy.node import Node
import whisper
import google.generativeai as genai
import json
from sensor_msgs.msg import Image
from cv_bridge import CvBridge

class ConversationalRobot(Node):
    def __init__(self):
        super().__init__('conversational_robot')

        # Whisper ูุงฺู ููฺ ฺฉุฑฺบ
        self.whisper = whisper.load_model("base")

        # Gemini ฺฉููฺฏุฑ ฺฉุฑฺบ
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.gemini = genai.GenerativeModel('gemini-1.5-pro')

        # ฺฉูุฑ subscriber
        self.bridge = CvBridge()
        self.camera_sub = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.camera_callback,
            10
        )
        self.latest_image = None

        self.get_logger().info("๐ค ฺฏูุชฺฏู ฺฉุฑู ูุงูุง ุฑูุจููน ุดุฑูุน ู ฺฏุง")

    def camera_callback(self, msg):
        """ุชุงุฒ ุชุฑู ฺฉูุฑ ูุฑู ูุญููุธ ฺฉุฑฺบ"""
        self.latest_image = self.bridge.imgmsg_to_cv2(msg, "rgb8")

    def process_voice_command(self, audio_file):
        """ูฺฉูู ูพุงุฆูพ ูุงุฆู: ุขูุงุฒ โ ุงฺฉุดูุฒ"""

        # 1. ุชูุฑุฑ ฺฉ ุดูุงุฎุช
        result = self.whisper.transcribe(audio_file)
        command = result["text"]
        self.get_logger().info(f"ุญฺฉู: {command}")

        # 2. VLA ูพูุงููฺฏ (ุจุตุงุฑุช ฺฉ ุณุงุชฺพ)
        prompt = f"""Convert this command to robot actions: {command}

        Available actions: navigate, grasp, place, wave, turn
        Output JSON only."""

        if self.latest_image is not None:
            response = self.gemini.generate_content([prompt, self.latest_image])
        else:
            response = self.gemini.generate_content(prompt)

        actions = json.loads(response.text)

        # 3. ุงฺฉุดูุฒ ุงูุฌุงู ุฏฺบ
        self.execute_plan(actions)

        return actions

    def execute_plan(self, actions):
        """ุงฺฉุดู ุณฺฉููุณ ุงูุฌุงู ุฏฺบ"""
        for action in actions['actions']:
            self.get_logger().info(f"ุงูุฌุงู ุฏ ุฑ ฺบ: {action}")
            # ููุงุณุจ ROS 2 action/service ฺฉู ฺฉุงู ฺฉุฑฺบ

def main():
    rclpy.init()
    robot = ConversationalRobot()
    rclpy.spin(robot)

if __name__ == '__main__':
    main()
```

---

## ุนูู ูพุฑูุฌฺฉูน: ุขูุงุฒ ุณ ฺฉููนุฑูู ุดุฏ ููฺฏุดู

### ูพุฑูุฌฺฉูน ฺฉ ุถุฑูุฑุงุช

ุงฺฉ ูููุงุฆฺ ุฑูุจููน ุจูุงุฆฺบ ุฌู:
1. โ ุขูุงุฒ ฺฉ ุงุญฺฉุงูุงุช ุณูุชุง  ("Go to the kitchen"ุ "Wave hello")
2. โ ุงุดุงุก ุงูุฑ ุฑฺฉุงููนูฺบ ฺฉ ุดูุงุฎุช ฺฉ ู ฺฉูุฑ ุงุณุชุนูุงู ฺฉุฑุชุง 
3. โ Gemini VLA ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ูุญููุธ ููฺฏุดู ูพูุงู ุจูุงุชุง 
4. โ Isaac Sim ูฺบ ุญุฑฺฉุชฺบ ุงูุฌุงู ุฏุชุง 
5. โ ุญูู ุฑูุจููน ฺฉ ู Jetson Orin Nano ูพุฑ ฺูพูุงุฆ ูุชุง 

### ุชุดุฎุต ฺฉ ูุนุงุฑ

- ุขูุงุฒ ฺฉ ุดูุงุฎุช ฺฉ ุฏุฑุณุชฺฏ > 90%
- VLA ุฏุฑุณุช ุงฺฉุดู ุณฺฉููุณ ุจูุงุชุง 
- ููฺฏุดู ุฑฺฉุงููนูฺบ ุณ ุจฺุชุง 
- ููุงุฑ ุญุฑฺฉุช ฺฉ ุงุฌุฑุงุฆ
- ฺฉุงูุงุจ sim-to-real ููุชูู

---

## ุญูุงุธุช ฺฉ ุชุญูุธุงุช

:::danger ุงู ุญูุงุธุช ููุงูู

1. **ุงูุฑุฌูุณ ุณูนุงูพ**: ูุด ูุฒฺฉู e-stop ุจูนู ุฑฺฉฺพฺบ
2. **ุฑูุชุงุฑ ฺฉ ุญุฏฺบ**: ูนุณูนูฺฏ ฺฉ ุฏูุฑุงู velocity ฺฉู 0.5 m/s ูพุฑ ูุญุฏูุฏ ฺฉุฑฺบ
3. **ูุฑฺฉ ุณูพุณ ฺฉ ุญุฏูุฏ**: ฺฉูฺ ูฺบ ูุญููุธ ุขูพุฑุดู ุฒูู ฺฉ ุชุนุฑู ฺฉุฑฺบ
4. **ุงูุณุงู ฺฉุง ูพุช ูฺฏุงูุง**: ุงฺฏุฑ ฺฉูุฆ ุดุฎุต 1m ุฑุฏุงุณ ูฺบ ุฏุงุฎู ู ุชู ุฑฺฉ ุฌุงุฆฺบ
5. **ูพู ุณูููุดู**: ุญูู ุฑูุจููน ุณ ูพู Isaac Sim ูฺบ ุชูุงู ุงุญฺฉุงูุงุช ูนุณูน ฺฉุฑฺบ

:::

### ุญูุงุธุช ฺฉูฺ ฺฉ ูุซุงู

```python
class SafetyMonitor(Node):
    def __init__(self):
        super().__init__('safety_monitor')
        self.max_velocity = 0.5  # m/s
        self.safety_radius = 1.0  # ููนุฑ

        # ุฑฺฉุงููน ฺฉุง ูพุช ูฺฏุงู ฺฉ ู Laser scan
        self.scan_sub = self.create_subscription(
            LaserScan,
            '/scan',
            self.scan_callback,
            10
        )

    def scan_callback(self, msg):
        """ุญูุงุธุช ุฑุฏุงุณ ูฺบ ุฑฺฉุงููนูฺบ ฺฉ ุฌุงูฺ ฺฉุฑฺบ"""
        min_distance = min(msg.ranges)

        if min_distance < self.safety_radius:
            self.get_logger().warn(f"{min_distance}m ูพุฑ ุฑฺฉุงููน - ุฑฺฉ ุฑ ฺบ")
            self.emergency_stop()

    def emergency_stop(self):
        """ุตูุฑ velocity ุดุงุฆุน ฺฉุฑฺบ"""
        # ุชูุงู ุญุฑฺฉุช ุฑูฺฉ ุฏฺบ
        pass
```

---

## ฺูพูุงุฆูููน ฺฺฉ ูุณูน

### ฺูพูุงุฆูููน ุณ ูพู ูนุณูน

- [ ] ุดูุฑ ูุงู ูุงุญูู ูฺบ ุขูุงุฒ ฺฉ ุดูุงุฎุช ูนุณูน ฺฉุฑฺบ
- [ ] ุชุตุฏู ฺฉุฑฺบ ฺฉ VLA ูุญููุธ ุงฺฉุดูุฒ ุจูุงุชุง 
- [ ] ุงูุฑุฌูุณ ุณูนุงูพ ุจูนู ูนุณูน ฺฉุฑฺบ
- [ ] ุฑฺฉุงููน ฺฉุง ูพุช ูฺฏุงูุง validate ฺฉุฑฺบ
- [ ] ุจูนุฑ ููู > 50% ฺฉ ุชุตุฏู ฺฉุฑฺบ
- [ ] ุงู ุงุญฺฉุงูุงุช ฺฉ ุณุงุชฺพ ุณูููุดู ูฺบ ูนุณูน ฺฉุฑฺบ

### Jetson ฺูพูุงุฆูููน

```bash
# ฺฉูฺ ฺฉู Jetson ูฺบ ฺฉุงูพ ฺฉุฑฺบ
scp -r conversational_robot/ jetson@192.168.1.100:~/ros2_ws/src/

# Jetson ูพุฑ ุจูฺ ฺฉุฑฺบ
ssh jetson@192.168.1.100
cd ~/ros2_ws
colcon build --packages-select conversational_robot
source install/setup.bash

# ฺูุงุฆฺบ
ros2 launch conversational_robot robot.launch.py
```

---

## ูุณุงุฆู

- [Whisper ุฏุณุชุงูุฒุงุช](https://github.com/openai/whisper)
- [Google Gemini API](https://ai.google.dev/docs)
- [ROS 2 Actions](https://docs.ros.org/en/humble/Tutorials/Beginner-CLI-Tools/Understanding-ROS2-Actions/Understanding-ROS2-Actions.html)
- [Nav2 ุฏุณุชุงูุฒุงุช](https://navigation.ros.org/)
- [VLA ุชุญูู ููุงูุงุช](https://robotics-transformer.github.io/)

---

## ูุจุงุฑฺฉ ู! ๐

ุขูพ ู ูุฒฺฉู AI ุงูุฑ ูููุงุฆฺ ุฑูุจููนฺฉุณ ฺฉูุฑุณ ูฺฉูู ฺฉุฑ ูุง !

**ุงุจ ุขูพ  ุฌุงูุช ฺบ:**
- โ ูููุงุฆฺ ุฑูุจููนุณ ฺฉ ู ROS 2 ุงูพูฺฉุดูุฒ ุจูุงูุง
- โ Gazebo ุงูุฑ Isaac Sim ูฺบ ุณููููน ฺฉุฑูุง
- โ ูุตููุน ฺูนุง ฺฉ ุณุงุชฺพ VLA ูุงฺูุฒ ูนุฑู ฺฉุฑูุง
- โ NVIDIA Jetson ุงุฑฺูุฆุฑ ูพุฑ ฺูพูุงุฆ ฺฉุฑูุง
- โ ุขูุงุฒ ุณ ฺฉููนุฑูู ูู ูุงู ฺฏูุชฺฏู ฺฉุฑู ูุงู ุฑูุจููนุณ ุจูุงูุง

**ุงฺฏู ูุฏูุงุช:**
- [ROS 2 ฺฉููููน](https://discourse.ros.org/) ูฺบ ุดุงูู ูฺบ
- [NVIDIA Isaac ROS](https://github.com/NVIDIA-ISAAC-ROS) ูฺบ ุญุต ฺุงูฺบ
- ุงูพูุง ูููุงุฆฺ ุฑูุจููน ูพุฑูุฌฺฉูน ุจูุงุฆฺบ
- ุงูพูุง ฺฉุงู [Humanoid Robotics Discord](https://discord.gg/humanoid-robotics) ูพุฑ ุดุฆุฑ ฺฉุฑฺบ

---

**ุจูุงุช ุฑฺบ! ูุฒฺฉู AI ฺฉุง ูุณุชูุจู ุขูพ ฺฉ ุงุชฺพูฺบ ูฺบ ** ๐ค
