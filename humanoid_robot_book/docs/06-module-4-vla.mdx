---
id: module-4-vla
title: "Module 4: Vision-Language-Action (VLA)"
sidebar_label: "Module 4: VLA"
sidebar_position: 6
---

# Module 4: Vision-Language-Action (VLA)

## Overview

**Focus**: The convergence of LLMs and Robotics

Vision-Language-Action (VLA) models represent the cutting edge of embodied AIâ€”systems that can understand natural language commands, perceive their environment through vision, and execute physical actions. This module teaches you to build conversational robots that respond to voice commands and perform complex tasks autonomously.

---

## Learning Objectives

By the end of this module, you will:

1. âœ… Integrate speech recognition with OpenAI Whisper
2. âœ… Use LLMs (GPT-4) for cognitive planning
3. âœ… Implement vision-based object detection
4. âœ… Translate natural language to ROS 2 actions
5. âœ… Build the Capstone Project: The Autonomous Humanoid
6. âœ… Deploy multi-modal interaction systems

---

## The VLA Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Voice Commandâ”‚â”€â”€â”€â”€â–¶â”‚  Whisper   â”‚â”€â”€â”€â”€â–¶â”‚   GPT-4      â”‚â”€â”€â”€â”€â–¶â”‚ ROS 2      â”‚
â”‚ "Clean room" â”‚     â”‚  (Speech   â”‚     â”‚  (Planning)  â”‚     â”‚ Actions    â”‚
â”‚              â”‚     â”‚  to Text)  â”‚     â”‚              â”‚     â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â”‚
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  Vision System  â”‚
                                        â”‚  (Object        â”‚
                                        â”‚   Detection)    â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component 1: Voice-to-Text with OpenAI Whisper

Whisper is OpenAI's state-of-the-art speech recognition model.

### Installing Whisper

```bash
pip install openai-whisper
```

### Basic Usage

```python
import whisper

# Load model (options: tiny, base, small, medium, large)
model = whisper.load_model("base")

# Transcribe audio
result = model.transcribe("audio.mp3")
print(result["text"])
```

---

### Real-Time ROS 2 Integration

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from audio_common_msgs.msg import AudioData
import whisper
import numpy as np
import io
import wave

class VoiceCommandNode(Node):
    def __init__(self):
        super().__init__('voice_command_node')

        # Load Whisper model
        self.model = whisper.load_model("base")

        # Subscribe to microphone audio
        self.audio_sub = self.create_subscription(
            AudioData,
            '/audio',
            self.audio_callback,
            10
        )

        # Publish transcribed commands
        self.command_pub = self.create_publisher(
            String,
            '/voice_command',
            10
        )

        self.audio_buffer = []
        self.buffer_duration = 3.0  # seconds

    def audio_callback(self, msg):
        # Accumulate audio data
        self.audio_buffer.extend(msg.data)

        # Process when buffer is full
        if len(self.audio_buffer) >= 48000 * self.buffer_duration:
            self.process_audio()
            self.audio_buffer = []

    def process_audio(self):
        # Convert to numpy array
        audio_data = np.array(self.audio_buffer, dtype=np.float32)
        audio_data = audio_data / 32768.0  # Normalize

        # Transcribe with Whisper
        result = self.model.transcribe(
            audio_data,
            language='en',
            fp16=False
        )

        command = result["text"].strip()

        if command:
            self.get_logger().info(f'Voice command: {command}')

            # Publish command
            msg = String()
            msg.data = command
            self.command_pub.publish(msg)

def main(args=None):
    rclpy.init(args=args)
    node = VoiceCommandNode()
    rclpy.spin(node)
    rclpy.shutdown()
```

---

## Component 2: Cognitive Planning with GPT-4

LLMs can translate high-level commands into sequences of robot actions.

### Task Decomposition Example

**Input**: "Clean the room"

**GPT-4 Output**:
```json
{
  "task": "clean_room",
  "steps": [
    {"action": "navigate_to", "target": "table", "priority": 1},
    {"action": "detect_objects", "category": "trash", "priority": 2},
    {"action": "pick_object", "object_id": "trash_1", "priority": 3},
    {"action": "navigate_to", "target": "trash_bin", "priority": 4},
    {"action": "place_object", "target": "trash_bin", "priority": 5},
    {"action": "return_to", "target": "charging_station", "priority": 6}
  ]
}
```

---

### ROS 2 Integration with GPT-4

```python
import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from std_msgs.msg import String
from geometry_msgs.msg import PoseStamped
from nav2_msgs.action import NavigateToPose
import openai
import json

class CognitivePlannerNode(Node):
    def __init__(self):
        super().__init__('cognitive_planner')

        # OpenAI client
        self.client = openai.OpenAI()

        # Subscribe to voice commands
        self.command_sub = self.create_subscription(
            String,
            '/voice_command',
            self.command_callback,
            10
        )

        # Navigation action client
        self.nav_client = ActionClient(
            self,
            NavigateToPose,
            'navigate_to_pose'
        )

        # Known locations
        self.locations = {
            "table": {"x": 2.0, "y": 3.0},
            "trash_bin": {"x": -1.0, "y": -2.0},
            "charging_station": {"x": 0.0, "y": 0.0}
        }

    def command_callback(self, msg):
        command = msg.data
        self.get_logger().info(f'Processing command: {command}')

        # Get action plan from GPT-4
        plan = self.generate_plan(command)

        # Execute plan
        self.execute_plan(plan)

    def generate_plan(self, command):
        """Use GPT-4 to generate action plan."""

        system_prompt = """You are a robot task planner. Given a high-level command,
        break it down into a sequence of robot actions. Available actions:
        - navigate_to(target): Move to a location
        - detect_objects(category): Find objects in the environment
        - pick_object(object_id): Pick up an object
        - place_object(target): Place object at target

        Available locations: table, trash_bin, charging_station

        Output JSON format:
        {
          "task": "task_name",
          "steps": [
            {"action": "navigate_to", "target": "location", "priority": 1},
            ...
          ]
        }
        """

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": command}
            ],
            response_format={"type": "json_object"}
        )

        plan_json = response.choices[0].message.content
        plan = json.loads(plan_json)

        self.get_logger().info(f'Generated plan: {json.dumps(plan, indent=2)}')
        return plan

    def execute_plan(self, plan):
        """Execute the action plan step by step."""

        for step in sorted(plan["steps"], key=lambda x: x["priority"]):
            action = step["action"]

            if action == "navigate_to":
                self.navigate_to(step["target"])
            elif action == "detect_objects":
                self.detect_objects(step["category"])
            elif action == "pick_object":
                self.pick_object(step["object_id"])
            elif action == "place_object":
                self.place_object(step["target"])

    def navigate_to(self, target):
        """Navigate to a target location."""

        if target not in self.locations:
            self.get_logger().error(f'Unknown location: {target}')
            return

        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.pose.position.x = self.locations[target]["x"]
        goal_msg.pose.pose.position.y = self.locations[target]["y"]
        goal_msg.pose.pose.orientation.w = 1.0

        self.get_logger().info(f'Navigating to {target}...')

        self.nav_client.wait_for_server()
        future = self.nav_client.send_goal_async(goal_msg)
        future.add_done_callback(self.goal_response_callback)

    def goal_response_callback(self, future):
        goal_handle = future.result()

        if not goal_handle.accepted:
            self.get_logger().error('Goal rejected')
            return

        self.get_logger().info('Goal accepted')
        result_future = goal_handle.get_result_async()
        result_future.add_done_callback(self.get_result_callback)

    def get_result_callback(self, future):
        result = future.result().result
        self.get_logger().info('Navigation completed')
```

---

## Component 3: Vision-Based Object Detection

Integrate YOLO or other object detection models for visual perception.

### YOLOv8 with ROS 2

```python
from ultralytics import YOLO
import cv2
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray, Detection2D
from cv_bridge import CvBridge

class ObjectDetectorNode(Node):
    def __init__(self):
        super().__init__('object_detector')

        # Load YOLO model
        self.model = YOLO('yolov8n.pt')

        # CV Bridge
        self.bridge = CvBridge()

        # Subscribe to camera
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10
        )

        # Publish detections
        self.detection_pub = self.create_publisher(
            Detection2DArray,
            '/detections',
            10
        )

    def image_callback(self, msg):
        # Convert ROS Image to OpenCV
        cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")

        # Run inference
        results = self.model(cv_image)

        # Create detection message
        detection_array = Detection2DArray()
        detection_array.header = msg.header

        for result in results:
            for box in result.boxes:
                detection = Detection2D()

                # Bounding box
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                detection.bbox.center.x = (x1 + x2) / 2
                detection.bbox.center.y = (y1 + y2) / 2
                detection.bbox.size_x = x2 - x1
                detection.bbox.size_y = y2 - y1

                # Class and confidence
                detection.results.append({
                    "id": int(box.cls[0]),
                    "score": float(box.conf[0])
                })

                detection_array.detections.append(detection)

        # Publish detections
        self.detection_pub.publish(detection_array)
```

---

## Capstone Project: The Autonomous Humanoid

### Project Overview

Build a complete autonomous humanoid system that:

1. **Receives voice commands** ("Clean the room", "Bring me water")
2. **Plans action sequences** using GPT-4
3. **Navigates autonomously** using Nav2
4. **Detects objects** with vision models
5. **Manipulates objects** with arm control
6. **Provides feedback** via speech synthesis

---

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Autonomous Humanoid System                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Whisper    â”‚â”€â”€â”€â”€â”€â–¶â”‚    GPT-4     â”‚â”€â”€â”€â”€â”€â–¶â”‚   Nav2    â”‚ â”‚
â”‚  â”‚ (Voice Input)â”‚      â”‚  (Planning)  â”‚      â”‚(Navigation)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                               â”‚
â”‚                              â–¼                               â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                     â”‚  YOLOv8/CLIP    â”‚                     â”‚
â”‚                     â”‚ (Object Detect) â”‚                     â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                              â”‚                               â”‚
â”‚                              â–¼                               â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                     â”‚   MoveIt2       â”‚                     â”‚
â”‚                     â”‚(Manipulation)   â”‚                     â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Requirements

**Hardware:**
- RTX 4080+ workstation for simulation
- NVIDIA Jetson Orin for deployment
- Intel RealSense D435i camera
- ReSpeaker microphone array
- Unitree G1 humanoid (or simulation)

**Software:**
- Isaac Sim for simulation
- ROS 2 Humble
- Isaac ROS packages
- OpenAI API access

---

### Implementation Steps

#### Step 1: Voice Command Processing

```python
# voice_processor.launch.py
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        Node(
            package='audio_capture',
            executable='audio_capture_node',
            parameters=[{
                'device': 'plughw:CARD=seeed8micvoicec,DEV=0',
                'format': 1,
                'channels': 8,
                'depth': 16,
                'sample_rate': 16000
            }]
        ),
        Node(
            package='my_robot',
            executable='voice_command_node',
            output='screen'
        ),
    ])
```

#### Step 2: Cognitive Planning

```python
# Run cognitive planner
ros2 run my_robot cognitive_planner_node
```

#### Step 3: Perception Pipeline

```bash
# Launch Isaac ROS VSLAM + Object Detection
ros2 launch my_robot perception.launch.py
```

#### Step 4: Navigation

```bash
# Launch Nav2
ros2 launch my_robot navigation.launch.py
```

#### Step 5: Integration

```bash
# Launch complete system
ros2 launch my_robot autonomous_humanoid.launch.py
```

---

## Multi-Modal Interaction

### Speech Synthesis for Feedback

```python
from gtts import gTTS
import os

class RobotSpeaker(Node):
    def __init__(self):
        super().__init__('robot_speaker')

        self.status_sub = self.create_subscription(
            String,
            '/robot_status',
            self.status_callback,
            10
        )

    def status_callback(self, msg):
        self.speak(msg.data)

    def speak(self, text):
        # Generate speech
        tts = gTTS(text=text, lang='en')
        tts.save("output.mp3")

        # Play audio
        os.system("mpg321 output.mp3")
        os.remove("output.mp3")
```

---

## Assessment Rubric

### Capstone Project Grading (100 points)

| Category | Points | Requirements |
|----------|--------|--------------|
| **Voice Command Processing** | 20 | Whisper integration, accuracy >90% |
| **Cognitive Planning** | 25 | GPT-4 task decomposition, JSON output validation |
| **Navigation** | 20 | Autonomous navigation with obstacle avoidance |
| **Object Detection** | 15 | YOLOv8 integration, detection accuracy >80% |
| **Manipulation** | 10 | Pick-and-place functionality |
| **System Integration** | 10 | All components working together |

**Bonus** (+10): Sim-to-real deployment on physical robot

---

## Resources

- [OpenAI Whisper](https://github.com/openai/whisper)
- [GPT-4 API Documentation](https://platform.openai.com/docs/api-reference)
- [YOLOv8](https://github.com/ultralytics/ultralytics)
- [MoveIt2 Documentation](https://moveit.ros.org/)
- [Nav2 Documentation](https://navigation.ros.org/)

---

## Congratulations! ðŸŽ‰

You've completed the **Physical AI & Humanoid Robotics** course!

You now have the skills to:
- âœ… Build ROS 2 robot controllers
- âœ… Simulate robots in Gazebo and Isaac Sim
- âœ… Implement AI-powered perception
- âœ… Create conversational humanoid robots
- âœ… Deploy to real hardware

### What's Next?

- Join the ROS community
- Contribute to open-source robotics projects
- Build your own robot startup
- Research advanced topics (multi-robot systems, swarm robotics, human-robot collaboration)

**Welcome to the future of Physical AI!** ðŸ¤–
